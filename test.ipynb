{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "import torch.nn as nn\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils\n",
    "\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import test_i_sample, test_x_sample\n",
    "from my_dataset import MyDataset\n",
    "from tf_callback import images_to_probs, plot_classes_preds, plot_confusion_matrix, plot_to_image\n",
    "from metrics import runningScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = f'model_{input()}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0307_0435\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pjoin('models', model_name, 'params.json'), 'r') as f:\n",
    "    params_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_seed = 245\n",
    "# np_seed = 123\n",
    "# torch_seed = 321\n",
    "# torch_cuda_seed = 111\n",
    "# learning_rate = 0.001\n",
    "step = params_dict['step']\n",
    "split_test = params_dict['split_test']\n",
    "path_data = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_shape=(401,701)\n",
    "iline, xline = im_shape\n",
    "i_locations = np.arange(0, iline, step)\n",
    "x_locations = np.arange(0, xline, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_i_mask = [i not in i_locations for i in range(0, iline)]\n",
    "test_x_mask = [x not in x_locations for x in range(0, xline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic = np.load(pjoin(path_data,'train','train_seismic.npy'))\n",
    "labels  = np.load(pjoin(path_data,'train','train_labels.npy' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seismic_ix = seismic[test_i_mask][:,test_x_mask]\n",
    "test_labels_ix = labels[test_i_mask][:,test_x_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_shape = test_seismic_ix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(396, 693, 255)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем U-Net модель с энкодером resnet34\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\", # resnet18 \n",
    "    # encoder_weights=\"swsl\", # можно обучать с нуля\n",
    "    in_channels=1,\n",
    "    classes=6,  # Количество классов для сегментации\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(pjoin('models', model_name, f'best_0_{model_name}.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(396, 693, 255)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_cat_i = np.zeros((test_data_shape[0], test_data_shape[1], test_data_shape[2], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396/396 [06:10<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test_data_shape[0])):\n",
    "    im = test_seismic_ix[i]\n",
    "    lbl = to_categorical(test_labels_ix[i], num_classes=6)\n",
    "    _, lbl_cat, pred_lbl_cat = test_i_sample(model, im, lbl, False)\n",
    "    pred_labels_cat_i[i] = pred_lbl_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pjoin('models', model_name, 'pred_labels_cat_i.npy'), pred_labels_cat_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_cat_x = np.zeros((test_data_shape[0], test_data_shape[1], test_data_shape[2], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 693/693 [04:58<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test_data_shape[1])):\n",
    "    im = test_seismic_ix[:,i]\n",
    "    lbl = to_categorical(test_labels_ix[:,i], num_classes=6)\n",
    "    _, lbl_cat, pred_lbl_cat = test_x_sample(model, im, lbl, False)\n",
    "    pred_labels_cat_x[:,i,:,:] = pred_lbl_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pjoin('models', model_name, 'pred_labels_cat_x.npy'), pred_labels_cat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [smp.utils.losses.DiceLoss(), nn.CrossEntropyLoss()]\n",
    "metrics = [smp.utils.metrics.IoU(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(pjoin('models', model_name, 'runs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396/396 [00:53<00:00,  7.46it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test_data_shape[0])):\n",
    "    lbl = torch.from_numpy(np.expand_dims(np.moveaxis(to_categorical(test_labels_ix[i],num_classes=6), -1, 0), axis=0))\n",
    "    pred_i = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_i[i], -1, 0), axis=0))\n",
    "    pred_x = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_x[i], -1, 0), axis=0))\n",
    "\n",
    "    li = torch.round(loss[0](pred_i, lbl) + loss[1](pred_i, lbl), decimals=2)\n",
    "    mi = torch.round(metrics[0](pred_i, lbl), decimals=2)\n",
    "    lx = torch.round(loss[0](pred_x, lbl) + loss[1](pred_x, lbl), decimals=2)\n",
    "    mx = torch.round(metrics[0](pred_x, lbl), decimals=2)\n",
    "\n",
    "    writer.add_scalar('test/(i_algo) loss along i', li, i)\n",
    "    writer.add_scalar('test/(i_algo) metric along i', mi, i)\n",
    "    writer.add_scalar('test/(x_algo) loss along i', lx, i)\n",
    "    writer.add_scalar('test/(x_algo) metric along i', mx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 693/693 [00:50<00:00, 13.69it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test_data_shape[1])):\n",
    "    lbl = torch.from_numpy(np.expand_dims(np.moveaxis(to_categorical(test_labels_ix[:,i],num_classes=6), -1, 0), axis=0))\n",
    "    pred_i = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_i[:,i], -1, 0), axis=0))\n",
    "    pred_x = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_x[:,i], -1, 0), axis=0))\n",
    "\n",
    "    li = torch.round(loss[0](pred_i, lbl) + loss[1](pred_i, lbl), decimals=2)\n",
    "    mi = torch.round(metrics[0](pred_i, lbl), decimals=2)\n",
    "    lx = torch.round(loss[0](pred_x, lbl) + loss[1](pred_x, lbl), decimals=2)\n",
    "    mx = torch.round(metrics[0](pred_x, lbl), decimals=2)\n",
    "\n",
    "    writer.add_scalar('test/(i_algo) loss along x', li, i)\n",
    "    writer.add_scalar('test/(i_algo) metric along x', mi, i)\n",
    "    writer.add_scalar('test/(x_algo) loss along x', lx, i)\n",
    "    writer.add_scalar('test/(x_algo) metric along x', mx, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([396, 6, 693, 255])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lbl_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.13 GiB for an array with shape (419874840,) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m pred_labels_cat_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(pjoin(\u001b[39m'\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m'\u001b[39m, model_name, \u001b[39m'\u001b[39m\u001b[39mpred_labels_cat_i.npy\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m pred_labels_cat_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(pjoin(\u001b[39m'\u001b[39;49m\u001b[39mmodels\u001b[39;49m\u001b[39m'\u001b[39;49m, model_name, \u001b[39m'\u001b[39;49m\u001b[39mpred_labels_cat_x.npy\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "File \u001b[1;32mc:\\Users\\Ravil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\npyio.py:432\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39m\u001b[39m.\u001b[39mopen_memmap(file, mode\u001b[39m=\u001b[39mmmap_mode,\n\u001b[0;32m    430\u001b[0m                                   max_header_size\u001b[39m=\u001b[39mmax_header_size)\n\u001b[0;32m    431\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 432\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mformat\u001b[39;49m\u001b[39m.\u001b[39;49mread_array(fid, allow_pickle\u001b[39m=\u001b[39;49mallow_pickle,\n\u001b[0;32m    433\u001b[0m                                  pickle_kwargs\u001b[39m=\u001b[39;49mpickle_kwargs,\n\u001b[0;32m    434\u001b[0m                                  max_header_size\u001b[39m=\u001b[39;49mmax_header_size)\n\u001b[0;32m    435\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    436\u001b[0m     \u001b[39m# Try a pickle\u001b[39;00m\n\u001b[0;32m    437\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[1;32mc:\\Users\\Ravil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\format.py:790\u001b[0m, in \u001b[0;36mread_array\u001b[1;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    788\u001b[0m     \u001b[39mif\u001b[39;00m isfileobj(fp):\n\u001b[0;32m    789\u001b[0m         \u001b[39m# We can use the fast fromfile() function.\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39;49mfromfile(fp, dtype\u001b[39m=\u001b[39;49mdtype, count\u001b[39m=\u001b[39;49mcount)\n\u001b[0;32m    791\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    792\u001b[0m         \u001b[39m# This is not a real file. We have to read it the\u001b[39;00m\n\u001b[0;32m    793\u001b[0m         \u001b[39m# memory-intensive way.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    801\u001b[0m         \u001b[39m# not correctly instantiate zero-width string dtypes; see\u001b[39;00m\n\u001b[0;32m    802\u001b[0m         \u001b[39m# https://github.com/numpy/numpy/pull/6430\u001b[39;00m\n\u001b[0;32m    803\u001b[0m         array \u001b[39m=\u001b[39m numpy\u001b[39m.\u001b[39mndarray(count, dtype\u001b[39m=\u001b[39mdtype)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.13 GiB for an array with shape (419874840,) and data type float64"
     ]
    }
   ],
   "source": [
    "pred_labels_cat_i = np.load(pjoin('models', model_name, 'pred_labels_cat_i.npy'))\n",
    "pred_labels_cat_x = np.load(pjoin('models', model_name, 'pred_labels_cat_x.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3358998720 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m lbl_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mmoveaxis(to_categorical(test_labels_ix,num_classes\u001b[39m=\u001b[39m\u001b[39m6\u001b[39m), (\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)))\n\u001b[0;32m      4\u001b[0m pred_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(np\u001b[39m.\u001b[39mmoveaxis(pred_labels_cat_x, (\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), (\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)))\n\u001b[1;32m----> 6\u001b[0m li \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(loss[\u001b[39m0\u001b[39;49m](pred_i, lbl_i) \u001b[39m+\u001b[39m loss[\u001b[39m1\u001b[39m](pred_i, lbl_i), decimals\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      7\u001b[0m mi \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(metrics[\u001b[39m0\u001b[39m](pred_i, lbl_i), decimals\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[0;32m      8\u001b[0m lx \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mround(loss[\u001b[39m0\u001b[39m](pred_x, lbl_x) \u001b[39m+\u001b[39m loss[\u001b[39m1\u001b[39m](pred_x, lbl_x), decimals\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Ravil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Ravil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\segmentation_models_pytorch\\utils\\losses.py:36\u001b[0m, in \u001b[0;36mDiceLoss.forward\u001b[1;34m(self, y_pr, y_gt)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, y_pr, y_gt):\n\u001b[0;32m     35\u001b[0m     y_pr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation(y_pr)\n\u001b[1;32m---> 36\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m F\u001b[39m.\u001b[39;49mf_score(\n\u001b[0;32m     37\u001b[0m         y_pr,\n\u001b[0;32m     38\u001b[0m         y_gt,\n\u001b[0;32m     39\u001b[0m         beta\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbeta,\n\u001b[0;32m     40\u001b[0m         eps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m     41\u001b[0m         threshold\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     42\u001b[0m         ignore_channels\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_channels,\n\u001b[0;32m     43\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ravil\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\segmentation_models_pytorch\\utils\\functional.py:57\u001b[0m, in \u001b[0;36mf_score\u001b[1;34m(pr, gt, beta, eps, threshold, ignore_channels)\u001b[0m\n\u001b[0;32m     54\u001b[0m pr \u001b[39m=\u001b[39m _threshold(pr, threshold\u001b[39m=\u001b[39mthreshold)\n\u001b[0;32m     55\u001b[0m pr, gt \u001b[39m=\u001b[39m _take_channels(pr, gt, ignore_channels\u001b[39m=\u001b[39mignore_channels)\n\u001b[1;32m---> 57\u001b[0m tp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(gt \u001b[39m*\u001b[39;49m pr)\n\u001b[0;32m     58\u001b[0m fp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(pr) \u001b[39m-\u001b[39m tp\n\u001b[0;32m     59\u001b[0m fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msum(gt) \u001b[39m-\u001b[39m tp\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3358998720 bytes."
     ]
    }
   ],
   "source": [
    "lbl_i = torch.from_numpy(np.moveaxis(to_categorical(test_labels_ix,num_classes=6), -1, 1))\n",
    "pred_i = torch.from_numpy(np.moveaxis(pred_labels_cat_i, -1, 1))\n",
    "lbl_x = torch.from_numpy(np.moveaxis(to_categorical(test_labels_ix,num_classes=6), (1, -1), (0, 1)))\n",
    "pred_x = torch.from_numpy(np.moveaxis(pred_labels_cat_x, (1, -1), (0, 1)))\n",
    "\n",
    "li = torch.round(loss[0](pred_i, lbl_i) + loss[1](pred_i, lbl_i), decimals=2)\n",
    "mi = torch.round(metrics[0](pred_i, lbl_i), decimals=2)\n",
    "lx = torch.round(loss[0](pred_x, lbl_x) + loss[1](pred_x, lbl_x), decimals=2)\n",
    "mx = torch.round(metrics[0](pred_x, lbl_x), decimals=2)\n",
    "\n",
    "print(f'i_algo: loss={li}, metric={mi}')\n",
    "print(f'x_algo: loss={lx}, metric={mx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((392, 686, 255), (392, 686, 255, 6), (392, 686, 255, 6))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_ix.shape, pred_labels_cat_i.shape, pred_labels_cat_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_i = np.argmax(pred_labels_cat_i, axis=-1)\n",
    "pred_labels_x = np.argmax(pred_labels_cat_x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((392, 686, 255), (392, 686, 255))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels_i.shape, pred_labels_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_metrics_test = runningScore(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['Pixel Acc', 'Mean Class Acc', 'Freq Weighted IoU', 'Mean IoU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Acc: \t0.973998024865923\n",
      "Mean Class Acc: \t0.9070583471725234\n",
      "Freq Weighted IoU: \t0.9504360344946858\n",
      "Mean IoU: \t0.8637697212892076\n",
      "Class[0] Accuracy:\t0.992789743170252\n",
      "Class[0] IoU:     \t0.9801244318166096\n",
      "Class[1] Accuracy:\t0.9438319127143\n",
      "Class[1] IoU:     \t0.9022877849231831\n",
      "Class[2] Accuracy:\t0.9893851310728955\n",
      "Class[2] IoU:     \t0.9740207627042012\n",
      "Class[3] Accuracy:\t0.9386464823429194\n",
      "Class[3] IoU:     \t0.8858961846746283\n",
      "Class[4] Accuracy:\t0.904464824182109\n",
      "Class[4] IoU:     \t0.7901789602697624\n",
      "Class[5] Accuracy:\t0.6732319895526655\n",
      "Class[5] IoU:     \t0.6501102033468612\n"
     ]
    }
   ],
   "source": [
    "running_metrics_test.update(test_labels_ix, pred_labels_i)\n",
    "score, class_iu = running_metrics_test.get_scores()\n",
    "running_metrics_test.reset()\n",
    "\n",
    "test_res_dict_i = {}\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    _s = score[f'{m}: ']\n",
    "    print(f'{m}: \\t{_s}')\n",
    "    test_res_dict_i[f'{m}'] = _s\n",
    "\n",
    "for i, _ca in enumerate(score['Class Accuracy: ']):\n",
    "    print(f'Class[{i}] Accuracy:\\t{_ca}')\n",
    "    test_res_dict_i[f'Class[{i}] Accuracy'] = _ca\n",
    "\n",
    "    print(f'Class[{i}] IoU:     \\t{class_iu[i]}')\n",
    "    test_res_dict_i[f'Class[{i}] IoU'] = class_iu[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Acc: \t0.9717394829651977\n",
      "Mean Class Acc: \t0.8921466915087365\n",
      "Freq Weighted IoU: \t0.9463613778218636\n",
      "Mean IoU: \t0.8462412199406376\n",
      "Class[0] Accuracy:\t0.9937088089132197\n",
      "Class[0] IoU:     \t0.980925161754491\n",
      "Class[1] Accuracy:\t0.9460882224851264\n",
      "Class[1] IoU:     \t0.9025041178486072\n",
      "Class[2] Accuracy:\t0.9874127408144003\n",
      "Class[2] IoU:     \t0.9699971429445104\n",
      "Class[3] Accuracy:\t0.93390328320508\n",
      "Class[3] IoU:     \t0.8779610783324893\n",
      "Class[4] Accuracy:\t0.892359972137966\n",
      "Class[4] IoU:     \t0.7663971774347683\n",
      "Class[5] Accuracy:\t0.599407121496627\n",
      "Class[5] IoU:     \t0.5796626413289593\n"
     ]
    }
   ],
   "source": [
    "running_metrics_test.update(test_labels_ix, pred_labels_x)\n",
    "score, class_iu = running_metrics_test.get_scores()\n",
    "running_metrics_test.reset()\n",
    "\n",
    "test_res_dict_x = {}\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    _s = score[f'{m}: ']\n",
    "    print(f'{m}: \\t{_s}')\n",
    "    test_res_dict_x[f'{m}'] = _s\n",
    "\n",
    "for i, _ca in enumerate(score['Class Accuracy: ']):\n",
    "    print(f'Class[{i}] Accuracy:\\t{_ca}')\n",
    "    test_res_dict_x[f'Class[{i}] Accuracy'] = _ca\n",
    "\n",
    "    print(f'Class[{i}] IoU:     \\t{class_iu[i]}')\n",
    "    test_res_dict_x[f'Class[{i}] IoU'] = class_iu[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pjoin('models', model_name, 'test_res_dict_i.json'), 'w') as f:\n",
    "    json.dump(test_res_dict_i, f)\n",
    "with open(pjoin('models', model_name, 'test_res_dict_x.json'), 'w') as f:\n",
    "    json.dump(test_res_dict_x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_ix = np.argmax((pred_labels_cat_i+pred_labels_cat_x)*0.5, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixel Acc: \t0.9747437458948593\n",
      "Mean Class Acc: \t0.8997274596024494\n",
      "Freq Weighted IoU: \t0.9516404018943099\n",
      "Mean IoU: \t0.8605054461994571\n",
      "Class[0] Accuracy:\t0.9942745116032634\n",
      "Class[0] IoU:     \t0.9827456173386825\n",
      "Class[1] Accuracy:\t0.9479334462858505\n",
      "Class[1] IoU:     \t0.9091408633323318\n",
      "Class[2] Accuracy:\t0.9900642968186641\n",
      "Class[2] IoU:     \t0.972814781910353\n",
      "Class[3] Accuracy:\t0.9447758919536424\n",
      "Class[3] IoU:     \t0.8975790572705358\n",
      "Class[4] Accuracy:\t0.9019024962335069\n",
      "Class[4] IoU:     \t0.7935355071700427\n",
      "Class[5] Accuracy:\t0.6194141147197688\n",
      "Class[5] IoU:     \t0.6072168501747967\n"
     ]
    }
   ],
   "source": [
    "running_metrics_test.update(test_labels_ix, pred_labels_ix)\n",
    "score, class_iu = running_metrics_test.get_scores()\n",
    "running_metrics_test.reset()\n",
    "\n",
    "test_res_dict_ix = {}\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    _s = score[f'{m}: ']\n",
    "    print(f'{m}: \\t{_s}')\n",
    "    test_res_dict_ix[f'{m}'] = _s\n",
    "\n",
    "for i, _ca in enumerate(score['Class Accuracy: ']):\n",
    "    print(f'Class[{i}] Accuracy:\\t{_ca}')\n",
    "    test_res_dict_ix[f'Class[{i}] Accuracy'] = _ca\n",
    "\n",
    "    print(f'Class[{i}] IoU:     \\t{class_iu[i]}')\n",
    "    test_res_dict_ix[f'Class[{i}] IoU'] = class_iu[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pjoin('models', model_name, 'test_res_dict_ix.json'), 'w') as f:\n",
    "    json.dump(test_res_dict_ix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
