{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.backends import cudnn\n",
    "import torch.nn as nn\n",
    "from torcheval.metrics.functional import multiclass_accuracy\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "import segmentation_models_pytorch.utils\n",
    "\n",
    "import cv2\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import PIL.Image\n",
    "\n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import test_i_sample, test_x_sample\n",
    "from my_dataset import MyDataset\n",
    "from tf_callback import images_to_probs, plot_classes_preds, plot_confusion_matrix, plot_to_image\n",
    "from metrics import runningScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_0407_1009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0407_1009\n"
     ]
    }
   ],
   "source": [
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pjoin('models', model_name, 'params.json'), 'r') as f:\n",
    "    params_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python_seed = 245\n",
    "# np_seed = 123\n",
    "# torch_seed = 321\n",
    "# torch_cuda_seed = 111\n",
    "# learning_rate = 0.001\n",
    "step = params_dict['step']\n",
    "split_test = params_dict['split_test']\n",
    "path_data = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_shape=(401,701)\n",
    "iline, xline = im_shape\n",
    "i_locations = np.arange(0, iline, step)\n",
    "x_locations = np.arange(0, xline, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_i_mask = [i not in i_locations for i in range(0, iline)]\n",
    "test_x_mask = [x not in x_locations for x in range(0, xline)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seismic = np.load(pjoin(path_data,'train','train_seismic.npy'))\n",
    "labels  = np.load(pjoin(path_data,'train','train_labels.npy' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniq, cnts = np.unique(labels, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "prcnts = np.round(100*cnts/np.sum(cnts),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.09%\t11.89%\t48.59%\t6.64%\t3.28%\t1.51%\t"
     ]
    }
   ],
   "source": [
    "for p in prcnts:\n",
    "    print(f'{p}%', end='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 20137839, 1: 8519666, 2: 34831122, 3: 4760778, 4: 2350150, 5: 1081200}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(uniq, cnts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seismic_ix = seismic[test_i_mask][:,test_x_mask]\n",
    "test_labels_ix = labels[test_i_mask][:,test_x_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "del seismic\n",
    "del labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_shape = test_seismic_ix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392, 686, 255)\n"
     ]
    }
   ],
   "source": [
    "print(test_data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем U-Net модель с энкодером resnet34\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\", # resnet18 \n",
    "    # encoder_weights=\"swsl\", # можно обучать с нуля\n",
    "    in_channels=1,\n",
    "    classes=6,  # Количество классов для сегментации\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(pjoin('models', model_name, f'best_0_{model_name}.pth')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 686, 255)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_cat_i = np.zeros((test_data_shape[0], test_data_shape[1], test_data_shape[2], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [06:22<00:00,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test_data_shape[0])):\n",
    "    im = test_seismic_ix[i]\n",
    "    lbl = to_categorical(test_labels_ix[i], num_classes=6)\n",
    "    _, lbl_cat, pred_lbl_cat = test_i_sample(model, im, lbl, False)\n",
    "    pred_labels_cat_i[i] = pred_lbl_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pjoin('models', model_name, 'pred_labels_cat_i.npy'), pred_labels_cat_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_cat_x = np.zeros((test_data_shape[0], test_data_shape[1], test_data_shape[2], 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 686/686 [05:38<00:00,  2.03it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(test_data_shape[1])):\n",
    "    im = test_seismic_ix[:,i]\n",
    "    lbl = to_categorical(test_labels_ix[:,i], num_classes=6)\n",
    "    _, lbl_cat, pred_lbl_cat = test_x_sample(model, im, lbl, False)\n",
    "    pred_labels_cat_x[:,i,:,:] = pred_lbl_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(pjoin('models', model_name, 'pred_labels_cat_x.npy'), pred_labels_cat_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [smp.utils.losses.DiceLoss(), nn.CrossEntropyLoss()]\n",
    "metrics = [smp.utils.metrics.IoU(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_labels_cat_i = np.load(pjoin('models', model_name, 'pred_labels_cat_i.npy'))\n",
    "# pred_labels_cat_x = np.load(pjoin('models', model_name, 'pred_labels_cat_x.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(392, 686, 255, 6)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels_cat_i.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(pjoin('models', model_name, 'runs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(test_data_shape[0])):\n",
    "#     lbl = torch.from_numpy(np.expand_dims(np.moveaxis(to_categorical(test_labels_ix[i],num_classes=6), -1, 0), axis=0))\n",
    "#     pred_i = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_i[i], -1, 0), axis=0))\n",
    "#     pred_x = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_x[i], -1, 0), axis=0))\n",
    "\n",
    "#     li = torch.round(loss[0](pred_i, lbl) + loss[1](pred_i, lbl), decimals=2)\n",
    "#     mi = torch.round(metrics[0](pred_i, lbl), decimals=2)\n",
    "#     lx = torch.round(loss[0](pred_x, lbl) + loss[1](pred_x, lbl), decimals=2)\n",
    "#     mx = torch.round(metrics[0](pred_x, lbl), decimals=2)\n",
    "\n",
    "#     writer.add_scalar('test/(i_algo) loss along i', li, i)\n",
    "#     writer.add_scalar('test/(i_algo) metric along i', mi, i)\n",
    "#     writer.add_scalar('test/(x_algo) loss along i', lx, i)\n",
    "#     writer.add_scalar('test/(x_algo) metric along i', mx, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in tqdm(range(test_data_shape[1])):\n",
    "#     lbl = torch.from_numpy(np.expand_dims(np.moveaxis(to_categorical(test_labels_ix[:,i],num_classes=6), -1, 0), axis=0))\n",
    "#     pred_i = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_i[:,i], -1, 0), axis=0))\n",
    "#     pred_x = torch.from_numpy(np.expand_dims(np.moveaxis(pred_labels_cat_x[:,i], -1, 0), axis=0))\n",
    "\n",
    "#     li = torch.round(loss[0](pred_i, lbl) + loss[1](pred_i, lbl), decimals=2)\n",
    "#     mi = torch.round(metrics[0](pred_i, lbl), decimals=2)\n",
    "#     lx = torch.round(loss[0](pred_x, lbl) + loss[1](pred_x, lbl), decimals=2)\n",
    "#     mx = torch.round(metrics[0](pred_x, lbl), decimals=2)\n",
    "\n",
    "#     writer.add_scalar('test/(i_algo) loss along x', li, i)\n",
    "#     writer.add_scalar('test/(i_algo) metric along x', mi, i)\n",
    "#     writer.add_scalar('test/(x_algo) loss along x', lx, i)\n",
    "#     writer.add_scalar('test/(x_algo) metric along x', mx, i)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lbl_i = torch.from_numpy(np.moveaxis(to_categorical(test_labels_ix,num_classes=6), -1, 1))\n",
    "# pred_i = torch.from_numpy(np.moveaxis(pred_labels_cat_i, -1, 1))\n",
    "# lbl_x = torch.from_numpy(np.moveaxis(to_categorical(test_labels_ix,num_classes=6), (1, -1), (0, 1)))\n",
    "# pred_x = torch.from_numpy(np.moveaxis(pred_labels_cat_x, (1, -1), (0, 1)))\n",
    "\n",
    "# li = torch.round(loss[0](pred_i, lbl_i) + loss[1](pred_i, lbl_i), decimals=2)\n",
    "# mi = torch.round(metrics[0](pred_i, lbl_i), decimals=2)\n",
    "# lx = torch.round(loss[0](pred_x, lbl_x) + loss[1](pred_x, lbl_x), decimals=2)\n",
    "# mx = torch.round(metrics[0](pred_x, lbl_x), decimals=2)\n",
    "\n",
    "# print(f'i_algo: loss={li}, metric={mi}')\n",
    "# print(f'x_algo: loss={lx}, metric={mx}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((392, 686, 255), (392, 686, 255, 6), (392, 686, 255, 6))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_ix.shape, pred_labels_cat_i.shape, pred_labels_cat_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_i = np.argmax(pred_labels_cat_i, axis=-1)\n",
    "pred_labels_x = np.argmax(pred_labels_cat_x, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((392, 686, 255), (392, 686, 255))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels_i.shape, pred_labels_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_metrics_test = runningScore(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['Pixel Acc', 'Mean Class Acc', 'Freq Weighted IoU', 'Mean IoU']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0407_1009\n",
      "Pixel Acc: \t0.97503898060682\n",
      "Mean Class Acc: \t0.9406687536408507\n",
      "Freq Weighted IoU: \t0.9526618634792456\n",
      "Mean IoU: \t0.890301082017122\n",
      "Class[0] Accuracy:\t0.9848572928154772\n",
      "Class[0] IoU:     \t0.973280509435295\n",
      "Class[1] Accuracy:\t0.9634516348766319\n",
      "Class[1] IoU:     \t0.9008507661921294\n",
      "Class[2] Accuracy:\t0.9860039206468098\n",
      "Class[2] IoU:     \t0.9783819197357092\n",
      "Class[3] Accuracy:\t0.9276622663291406\n",
      "Class[3] IoU:     \t0.8764050879081702\n",
      "Class[4] Accuracy:\t0.9172693426384813\n",
      "Class[4] IoU:     \t0.8029769541838312\n",
      "Class[5] Accuracy:\t0.8647680645385631\n",
      "Class[5] IoU:     \t0.8099112546475968\n"
     ]
    }
   ],
   "source": [
    "running_metrics_test.update(test_labels_ix, pred_labels_i)\n",
    "score, class_iu = running_metrics_test.get_scores()\n",
    "running_metrics_test.reset()\n",
    "\n",
    "test_res_dict_i = {}\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    _s = score[f'{m}: ']\n",
    "    print(f'{m}: \\t{_s}')\n",
    "    test_res_dict_i[f'{m}'] = _s\n",
    "\n",
    "for i, _ca in enumerate(score['Class Accuracy: ']):\n",
    "    print(f'Class[{i}] Accuracy:\\t{_ca}')\n",
    "    test_res_dict_i[f'Class[{i}] Accuracy'] = _ca\n",
    "\n",
    "    print(f'Class[{i}] IoU:     \\t{class_iu[i]}')\n",
    "    test_res_dict_i[f'Class[{i}] IoU'] = class_iu[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0407_1009\n",
      "Pixel Acc: \t0.9744187325075803\n",
      "Mean Class Acc: \t0.9175861639222856\n",
      "Freq Weighted IoU: \t0.9513348829192126\n",
      "Mean IoU: \t0.8721514802682048\n",
      "Class[0] Accuracy:\t0.9885908059999469\n",
      "Class[0] IoU:     \t0.9807383544956961\n",
      "Class[1] Accuracy:\t0.9642730705434569\n",
      "Class[1] IoU:     \t0.898167216086171\n",
      "Class[2] Accuracy:\t0.985776121263594\n",
      "Class[2] IoU:     \t0.9739216316204488\n",
      "Class[3] Accuracy:\t0.9311549817075911\n",
      "Class[3] IoU:     \t0.8837516039708136\n",
      "Class[4] Accuracy:\t0.9354134908324562\n",
      "Class[4] IoU:     \t0.8219182850130268\n",
      "Class[5] Accuracy:\t0.7003085131866689\n",
      "Class[5] IoU:     \t0.6744117904230719\n"
     ]
    }
   ],
   "source": [
    "running_metrics_test.update(test_labels_ix, pred_labels_x)\n",
    "score, class_iu = running_metrics_test.get_scores()\n",
    "running_metrics_test.reset()\n",
    "\n",
    "test_res_dict_x = {}\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    _s = score[f'{m}: ']\n",
    "    print(f'{m}: \\t{_s}')\n",
    "    test_res_dict_x[f'{m}'] = _s\n",
    "\n",
    "for i, _ca in enumerate(score['Class Accuracy: ']):\n",
    "    print(f'Class[{i}] Accuracy:\\t{_ca}')\n",
    "    test_res_dict_x[f'Class[{i}] Accuracy'] = _ca\n",
    "\n",
    "    print(f'Class[{i}] IoU:     \\t{class_iu[i]}')\n",
    "    test_res_dict_x[f'Class[{i}] IoU'] = class_iu[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pjoin('models', model_name, 'test_res_dict_i.json'), 'w') as f:\n",
    "    json.dump(test_res_dict_i, f)\n",
    "with open(pjoin('models', model_name, 'test_res_dict_x.json'), 'w') as f:\n",
    "    json.dump(test_res_dict_x, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pred_labels_i\n",
    "del pred_labels_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels_ix = np.argmax((pred_labels_cat_i+pred_labels_cat_x)*0.5, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_0407_1009\n",
      "Pixel Acc: \t0.9769334555979826\n",
      "Mean Class Acc: \t0.9292122315851173\n",
      "Freq Weighted IoU: \t0.9559228976346963\n",
      "Mean IoU: \t0.8872687127896667\n",
      "Class[0] Accuracy:\t0.9893585713170517\n",
      "Class[0] IoU:     \t0.9802952320898461\n",
      "Class[1] Accuracy:\t0.965960240992094\n",
      "Class[1] IoU:     \t0.907123297174404\n",
      "Class[2] Accuracy:\t0.9874406566697688\n",
      "Class[2] IoU:     \t0.9771343510075393\n",
      "Class[3] Accuracy:\t0.935316682874205\n",
      "Class[3] IoU:     \t0.8990066040231115\n",
      "Class[4] Accuracy:\t0.9402194691525891\n",
      "Class[4] IoU:     \t0.8281842110834328\n",
      "Class[5] Accuracy:\t0.7569777685049958\n",
      "Class[5] IoU:     \t0.7318685813596661\n"
     ]
    }
   ],
   "source": [
    "running_metrics_test.update(test_labels_ix, pred_labels_ix)\n",
    "score, class_iu = running_metrics_test.get_scores()\n",
    "running_metrics_test.reset()\n",
    "\n",
    "test_res_dict_ix = {}\n",
    "\n",
    "print(model_name)\n",
    "\n",
    "for i, m in enumerate(metric_names):\n",
    "    _s = score[f'{m}: ']\n",
    "    print(f'{m}: \\t{_s}')\n",
    "    test_res_dict_ix[f'{m}'] = _s\n",
    "\n",
    "for i, _ca in enumerate(score['Class Accuracy: ']):\n",
    "    print(f'Class[{i}] Accuracy:\\t{_ca}')\n",
    "    test_res_dict_ix[f'Class[{i}] Accuracy'] = _ca\n",
    "\n",
    "    print(f'Class[{i}] IoU:     \\t{class_iu[i]}')\n",
    "    test_res_dict_ix[f'Class[{i}] IoU'] = class_iu[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pjoin('models', model_name, 'test_res_dict_ix.json'), 'w') as f:\n",
    "    json.dump(test_res_dict_ix, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _s = summary(model, (1,256,256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.zeros(1, 1, 256, 256, dtype=torch.float, requires_grad=False)\n",
    "# out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchview import draw_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_graph = draw_graph(model, input_size=(1,1,256,256), expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_data = torch.randn(1, 1, 256, 256)  # Пример входных данных\n",
    "\n",
    "# # Определите выход модели, чтобы создать граф\n",
    "# output_data = model(input_data)\n",
    "\n",
    "# # Создайте граф\n",
    "# dot_graph = make_dot(output_data, params=dict(model.named_parameters()))\n",
    "# dot_graph.view()  # Откроется окно с изображением графа модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_dot(output_data).render(\"attached\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_graph = draw_graph(model, input_size=(1,1,256,256), expand_nested=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_graph.visual_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy_input = torch.randn(1, 1, 256, 256)\n",
    "# torch.onnx.export(model, dummy_input, 'model.onnx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import netron\n",
    "# netron.start('model.onnx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
